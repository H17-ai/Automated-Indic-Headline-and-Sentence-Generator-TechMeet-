{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRNUcNjJ6NGs1Q405qxmx9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0WqTq10aO0hC"},"outputs":[],"source":["#Basic Python and Machine learning libraries\n","import os, sys, warnings, random, time, re, math, string, demoji, emoji, copy\n","import pandas as pd\n","import numpy as np\n","import Levenshtein\n","import matplotlib.pyplot as plt\n","import networkx as nx\n","\n","from string import punctuation\n","from collections import Counter\n","from re import search\n","import seaborn as sns\n","from scipy import stats\n","from sklearn import metrics\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics\n","from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import SelectFromModel\n","\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer, WordNetLemmatizer\n","from nltk.tokenize.casual import casual_tokenize\n","from nltk.util import ngrams\n","\n","import spacy\n","# from spacy_langdetect import LanguageDetector\n","\n","warnings.filterwarnings('ignore')\n","demoji.download_codes()\n","\n","#tqdm with pandas\n","from tqdm import tqdm\n","tqdm.pandas()\n","from collections import Counter\n","\n","# sklearn data science models\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\n","from sklearn.svm import LinearSVC\n","import xgboost as xgb\n","\n","from wordsegment import load, segment\n"]},{"cell_type":"code","source":["def basic_clean(text):\n","    # text = emoji.demojize(text)\n","    text = str(text)\n","    text = demoji.replace_with_desc(text, sep = \" \") #Emoji to text\n","    \n","    text = BeautifulSoup(text, 'lxml').get_text() #links\n","    text = re.sub('https?://\\S+|www\\.\\S+', '', text) #links\n","    text = re.sub(' +', ' ', text) \n","    text = re.sub(\"^RT\", \"\", text) #remove RT\n","    text = re.sub(' +', ' ', text) \n","    text = re.sub(\"^QT\", \"\", text) #remove QT\n","    text = re.sub(' +', ' ', text) \n","    text = re.sub('\\n', \" \", text) #newlines\n","    text = re.sub(' +', ' ', text) \n","    text = str(text).lower() #makes everything lowercase\n","    emoji_pattern = re.compile(\"[\"\n","                          \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                          \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                          \"\\U0001F600-\\U0001F64F\"  # emoticons\n","                          \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                          \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","                          \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","                          \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","                          \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","                          \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","                          \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","                          \"\\U00002702-\\U000027B0\"  # Dingbats\n","                          \"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r' ', text)\n","    text = re.sub(' +', ' ', text) \n","    text = text.replace('@', ' ').replace('#',' ').replace('_',' ') #removes @ # _ from mentions and user-handles, does not remove the handle itself\n","    return text\n","  \n"],"metadata":{"id":"UT00dc2kPRtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def camel_case_split(identifier):\n","  matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n","  return \" \".join([m.group(0) for m in matches])\n","\n","def clean_all(text):\n","    text = basic_clean(text)\n","    text = camel_case_split(text)\n","\n","    return text\n","\n","def remove_duplicates(df, mode='tweet'):\n","    if mode=='tweet':\n","        COL_NAME = 'Tweet'\n","        THRESHOLD = 0.2\n","    elif mode == 'article':\n","        COL_NAME = 'Text'\n","        THRESHOLD = 0.05\n","    temp_df=copy.deepcopy(df[f'{COL_NAME}'].apply(clean_all))\n","    labels = temp_df.duplicated()    \n","    edit =[]\n","    print(\"Num of duplicates: \", labels.sum())\n","\n","    for i in range(len(labels)-1):\n","        for j in range(i+1,len(labels)):\n","            if labels.iloc[i] or labels.iloc[j]:\n","                continue\n","            dist = Levenshtein.distance(temp_df.iloc[i],temp_df.iloc[j])\n","            edit.append((dist, i, j))\n","    dumbs = []\n","    for a, b, c in edit:\n","        if a/max(len(temp_df.iloc[b]),len(temp_df.iloc[c])) < THRESHOLD:\n","            dumbs.append((b,c))\n","    G = nx.Graph()\n","    G.add_edges_from(dumbs)\n","    connected = list(nx.connected_components(G))\n","    for i,connect in enumerate(connected):\n","        connect = list(connect)\n","        for j in range(1,len(connect)):\n","            labels.iloc[connect[j]] = True\n","    final  = copy.copy(df[~labels])\n","    \n","    return final\n","\n","if __name__ == '__main__' :\n","    tweets = pd.read_csv('Data/labeled_tweet.csv')\n","    print(tweets.columns)\n","    tweets.text = tweets.text.apply(clean_all)\n","    print(tweets.columns)\n","    t1 = time.time()\n","    # new_df = remove_duplicates(tweets)\n","    t2 = time.time()\n","    # print(new_df.shape)\n","    print(t2-t1)"],"metadata":{"id":"i1_I2Mu8PRlg"},"execution_count":null,"outputs":[]}]}